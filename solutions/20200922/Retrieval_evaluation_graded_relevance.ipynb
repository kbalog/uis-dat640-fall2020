{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval evaluation, graded relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import math\n",
    "import pytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rankings produced for each query\n",
    "\n",
    "The key is the query ID, the value is a list of document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_rankings = {\n",
    "    'q1': [2, 1, 3, 4, 5, 6, 10, 7, 9, 8],\n",
    "    'q2': [1, 2, 9, 4, 5, 6, 7, 8, 3, 10],\n",
    "    'q3': [1, 7, 4, 5, 3, 6, 9, 8, 10, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth\n",
    "\n",
    "The key is the query ID, the value is a dictionary with (document ID, relevance) pairs. Relevance is measured on a 3-point scale: non-relevant (0), poor (1), good (2), excellent (3). Documents not listed here are non-relevant (relevance=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {\n",
    "    'q1': {4: 3, 1: 2, 2: 1},\n",
    "    'q2': {3: 3, 4: 3, 1: 2, 2: 1, 8: 1},\n",
    "    'q3': {1: 3, 4: 3, 7: 2, 5: 2, 6: 1, 8: 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing evaluation metrics\n",
    "\n",
    "Discounted cumulative gain at rank $k$ is computed as:\n",
    "\n",
    "$$DCG_k = rel_1 + \\sum_{i=2}^k\\frac{rel_i}{\\log_2 i}$$\n",
    "\n",
    "Normalized discounted cumulative gain at rank $k$ is computed as:\n",
    "\n",
    "$$NDCG_k = \\frac{DCG_k}{IDCG_k}$$\n",
    "\n",
    "where $IDCG_k$ is the $DCG_k$ score of an idealized (perfect) ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(relevances, k):\n",
    "    \"\"\"Computes DCG@k, given the corresponding relevance levels for a ranked list of documents.\n",
    "    \n",
    "    For example, given a ranking [2, 3, 1] where the relevance levels according to the ground \n",
    "    truth are {1:3, 2:4, 3:1}, the input list will be [4, 1, 3].\n",
    "    \n",
    "    Args:\n",
    "        relevances: List with the ground truth relevance levels corresponding to a ranked list of documents.\n",
    "        k: Rank cut-off.\n",
    "        \n",
    "    Returns:\n",
    "        DCG@k (float).\n",
    "    \"\"\"\n",
    "    dcg = relevances[0]\n",
    "    for i in range(1, min(k, len(relevances))): \n",
    "        dcg += relevances[i] / math.log(i + 1, 2)  # Note: Rank position is indexed from 1.\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(system_ranking, ground_truth, k=10):\n",
    "    \"\"\"Computes NDCG@k for a given system ranking.\n",
    "    \n",
    "    Args:\n",
    "        system_ranking: Ranked list of document IDs (from most to least relevant).\n",
    "        ground_truth: Dict with document ID: relevance level pairs. Document not present here are to be taken with relevance = 0.\n",
    "        k: Rank cut-off.\n",
    "    \n",
    "    Returns:\n",
    "        NDCG@k (float).\n",
    "    \"\"\"\n",
    "    relevances = []  # Holds corresponding relevance levels for the ranked docs.\n",
    "    for doc_id in system_ranking: \n",
    "        relevances.append(ground_truth.get(doc_id, 0))\n",
    "\n",
    "    # Relevance levels of the idealized ranking.\n",
    "    relevances_ideal = sorted([v for _, v in ground_truth.items()], reverse=True)\n",
    "    \n",
    "    return dcg(relevances, k) / dcg(relevances_ideal, k)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......                                                                             [100%]\n",
      "6 passed in 0.03s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"qid,k,correct_value\", [\n",
    "    ('q1', 5, 0.799),\n",
    "    ('q1', 10, 0.799),\n",
    "    ('q2', 5, 0.549),\n",
    "    ('q2', 10, 0.705),\n",
    "    ('q3', 5, 0.908),\n",
    "    ('q3', 10, 0.949),\n",
    "])\n",
    "def test_queries(qid, k, correct_value):    \n",
    "    assert ndcg(system_rankings[qid], ground_truth[qid], k) == pytest.approx(correct_value, rel=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "Please give (anonymous) feedback on this exercise by filling out [this form](https://forms.gle/2jPayczbFhEcC9K68)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
